###########################################################################
#  Copyright 2021 DataRobot Inc. All Rights Reserved.                     #
#                                                                         #
#  This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES       #
#  OR CONDITIONS OF ANY KIND, express or implied.                         #
###########################################################################


AWSTemplateFormatVersion: "2010-09-09"
Description: DataRobot CloudFormation stack to create the EC2 instances for the cluster. (qs-1xxxxxxxx) 
Metadata:
  QuickStartDocumentation:
    EntrypointName: Launch into an existing VPC
    Order: "5"
  QSLint:
    Exclusions: [ W9002, W9003, W9006 ]
  cfn-lint:
    config:
      ignore_checks:
        - E9101
        - W1020
Parameters:
  EC2InstanceSecurityProfile:
    Type: 'String'
    Description: EC2 Security Role
  EC2SecurityGroup:
    Type: 'AWS::EC2::SecurityGroup::Id'
    Description: Security group for EC2 allowed traffic
  MainStack:
    Type: String
    Description: Main Stack Name
  DRVersion:
    Type: String
    Description: DataRobot version to install
    AllowedPattern: '(\d{1,3})\.(\d{1,3})\.(\d{1,3})'
    Default: 7.0.2
  InstallationBucket:
    Type: String
    Description: CloudFormation repository S3 bucket name
  SSHKey:
    Type: "AWS::EC2::KeyPair::KeyName"
    Description: Key pair to use for connectivity
  SubnetId:
    Type: "AWS::EC2::Subnet::Id"
    Description: Subnet for cluster
  ImageId:
    Type: AWS::EC2::Image::Id
    Description: RHEL or CentOS AMI Id
  InstanceType:
    Type: String
    Description: AWS Instance Type
    AllowedValues: [ "t3.2xlarge", "m5.xlarge", "m5.2xlarge", "c5.xlarge", "c5.2xlarge", "r5.xlarge", "r5.2xlarge", "r5.4xlarge", "r5d.12xlarge", "r5.large" ]
  NodeVolumeSize:
    Type: Number
    Description: Hard drive size for Operating System
    AllowedValues: [ 40, 60, 80, 100 ]
    Default: 60
  Region:
    Type: String
    Description: Region to host cluster
    AllowedValues: [ "ap-south-1", "eu-west-1", "eu-west-2", "eu-west-3", "eu-north-1", "ap-northeast-1", "ap-northeast-2", "sa-east-1", "ca-central-1", "ap-southeast-1", "ap-southeast-2", "eu-central-1", "us-east-1", "us-east-2", "us-west-1", "us-west-2" ]
    Default: us-east-1
  Encrypted:
    Type: String
    Description: Enable encryption on the DataRobot Application Drive
    Default: true
    AllowedValues: [ true, false ]
  EncryptionKey:
    Type: String
    Description: AWS KMS key used to encrypt the DataRobot App drive at /opt/datarobot
    # Default: <some encryption key value>
  PublicIpAddress:
    Type: String
    Description: Should this node have a Public IP
    Default: false
    AllowedValues: [ true, false ]
  StorageDir:
    Type: String
    Description: Where to store the verte4x files in our S3 bucket
    Default: "datarobot_storage"
  LogServer:
    Type: String
    Description: Type of Log Aggrigation Server to configure. Current options are off or Elastic/Logstacsh/Kibana. Splunk is on the way.
    AllowedValues: [ off, ELK ]
    Default: off
  DataVolumeSize:
    Type: Number
    Description: Hard drive size for the DataRobot software
    AllowedValues: [ 40, 60, 80, 100, 200 ]
  TagDataVol:
    Type: String
    Description: Tag the data volume on this node for snapshotting
    Default: false
    AllowedValues: [ true, false ]

  BackupDir:
    Type: String
    Description: Where to store the secrets and data file backups
    Default: "backup_files"

  UseS3BucketBackups:
    Type: String
    Description: Where to store the secrets and data file backups
    Default: true
    AllowedValues: [ true, false ]

  S3BucketBackupSchedule:
    Type: String
    Description: "Cron tab formatted schedule. Default: '0 0 * * 1-5' or Monday - Friday at midnight"
    Default: "0 0 * * 1-5"
    # Default: "30 * * * *"
  ServerName:
    Type: String
    Description: "Logical Name for the Server"
  AmiSignalHandle:
    Type: String
    Description: "presigned url for ami creation handle"
    Default: " "


Conditions:
  UseKey: !And
    - !Equals [!Ref Encrypted, true]
    - !Not [!Equals [!Ref EncryptionKey, ""]]

Resources:
  EC2Instance:
    Type: "AWS::EC2::Instance"
    Metadata: 
      AWS::CloudFormation::Init: 
        config:
          packages:
            rpm:
              epel: "https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm"
            yum:
              #awscli: []
              unzip: []
          groups: 
            docker: {}
          files:
            # Give the datarobot user sudo access
            /etc/sudoers.d/datarobot:
              content: !Sub |
                datarobot ALL=(ALL) NOPASSWD: ALL
              mode: "000644"
              owner: "root"
              group: "root"
            # Set max open files and lock memory
            /etc/security/limits.d/20-nproc.conf:
              content: !Sub |
                # Default limit for number of user's processes to prevent
                # accidental fork bombs.
                # See rhbz #432903 for reasoning.

                *          soft    nproc     4096
                root       soft    nproc     unlimited
                datarobot  hard    nproc     32768
                datarobot  soft    nproc     32768
                datarobot  -       memlock   unlimited
                datarobot  -       nofile    65536
                datarobot  -       nproc     32768
              mode: "000644"
              owner: "root"
              group: "root"
            # Set this Docker service parameters to avoid 'Out of Memory' and 'Too Many Open Files' errors.
            /etc/systemd/system/docker.service.d/1-datarobot.conf:
              content: !Sub |
                [Service]
                LimitMEMLOCK=infinity
                LimitNOFILE=65536
                Restart=on-failure

              mode: "000644"
              owner: "root"
              group: "root"

            # Disable IPv6 in the config
            /etc/sysctl.d/70-ipv6.conf:
              content: !Sub |
                net.ipv6.conf.all.disable_ipv6 = 1
                net.ipv6.conf.default.disable_ipv6 = 1
              mode: "000644"
              owner: "root"
              group: "root"

          commands:
            0a-PackageInstallation:
              command: !Sub |



                echo "Installing the AWS cli v2"
                curl --silent --output /tmp/awscliv2.zip "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip"
                unzip /tmp/awscliv2.zip -d /tmp
                sudo /tmp/aws/install

              ignoreErrors: "false"
                          
            # 0a-PackageInstallation:
            #   command: !Sub |

            #     # Install python pip
            #     yum install -y python-pip

            #     # Install or Upgrade the required python packages
            #     pip install --upgrade pip awscli future boto3 argparse requests

            0b-TurnOffIPv6:
              command: !Sub |

                # Force new line in the log
                echo " "

                echo "removing ::1 from /etc/hosts "
                sed -i --follow-symlinks 's/::1/#::1/' /etc/hosts

                echo "Loading IPv6 networking config"
                sysctl --load /etc/sysctl.d/70-ipv6.conf

                echo "You should see 1's"
                sysctl --all | grep disable_ipv6

              test: test -s /etc/sysctl.d/70-ipv6.conf
              ignoreErrors: "false"

            1a-CreateDataRobotUser:
              command: !Sub |

                # Force new line in the log
                echo " "

                # Provision the datarobot user
                useradd --create-home -u 1234 datarobot
                usermod -aG docker datarobot

                echo "Created datarobot user:> $(id datarobot)"
                echo "max open files :> $(/usr/sbin/runuser -l datarobot -c 'ulimit -n')"
                echo "max locked memory:> $(/usr/sbin/runuser -l datarobot -c 'ulimit -l')"
                echo "max user processes:> $(/usr/sbin/runuser -l datarobot -c 'ulimit -u')"

              # Confirm datarobot does not exist
              test: test ! $(getent passwd datarobot) > /dev/null 2>&1
              ignoreErrors: "false"

            1b-GenerateKeys:
              command: !Sub |

                # Force new line in the log
                echo " "

                # Generate SSH Keys
                /usr/sbin/runuser -l datarobot -c 'ssh-keygen -t rsa -b 2048 -f /home/datarobot/.ssh/id_rsa -N ""'

                # Copy Public Key to authorized_keys
                /usr/sbin/runuser -l datarobot -c 'cat /home/datarobot/.ssh/id_rsa.pub | tee -a /home/datarobot/.ssh/authorized_keys'

                # Chmod authorized_keys
                /usr/sbin/runuser -l datarobot -c 'chmod 600 /home/datarobot/.ssh/authorized_keys'

                # Make sure the node can talk to it self
                echo -n "Ensuring node can talk to it self:>"
                /usr/sbin/runuser -l datarobot -c "ssh -o StrictHostKeyChecking=no -o LogLevel=ERROR $(curl --silent http://169.254.169.254/latest/meta-data/local-ipv4) date"

              test: test ! -s /home/datarobot/.ssh/id_rsa
              ignoreErrors: "false"
              env:
                AWS_REGION: !Ref Region
                MAIN_STACK: !Ref MainStack

            2a-SetSELinux:
              command: !Sub |
                # do not disable SELINUX for quickstart branch
                # Force new line in the log
                # echo " "

                # Set SELinux to Permissive
                # sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=permissive/' /etc/sysconfig/selinux
                # setenforce 0
                # echo "SELinux :> $(getenforce)"

              # Confirm SELinux = Permissive
              # test: "test $(getenforce) = 'Enforcing'"
              ignoreErrors: "true"
            
            2b-ConfigureSSL:
              command: !Sub |

                # Force new line in the log
                echo " "

                # Configure SSL and re-start the service
                sed -i 's/#PubkeyAuthentication yes/PubkeyAuthentication yes/' /etc/ssh/sshd_config
                sed -i 's/#RSAAuthentication yes/RSAAuthentication yes/' /etc/ssh/sshd_config
                systemctl restart sshd
                echo "Restarted sshd"

              test: "grep '#PubkeyAuthentication yes' /etc/ssh/sshd_config"
              ignoreErrors: "false"

            2c-MountFileSystem:
              command: !Sub |

                # Force new line in the log
                echo " "

                # Apply EXT4 File System to the new volume
                /usr/sbin/mkfs -t ext4 /dev/nvme1n1

                # Make the directory and mount the volume
                mkdir -p /opt/datarobot
                
                # Add entry into the fstab
                echo "$(file -s /dev/nvme1n1 | cut -f 8 -d ' ') /opt/datarobot ext4 defaults,nofail 0 0" | tee -a /etc/fstab

                # Mount the volume
                mount -a

              test: "lsblk | grep nvme1n1 | cut -d ' ' -f 16"
              ignoreErrors: "false"

            2d-MakeDockerDirectory:
              command: !Sub |

                # Force new line in the log
                echo " "

                mkdir -p /opt/datarobot/docker
                echo "Made Directory :> /opt/datarobot/docker"

                # Create the docker symlink
                ln -s /opt/datarobot/docker /var/lib/docker
                chown -h datarobot:datarobot /var/lib/docker
                chown -R datarobot:datarobot /opt/datarobot
                echo "Created Symlink :> $(ls -al /var/lib/docker)"

              ignoreErrors: "false"
              test: test ! -L /var/lib/docker      

            3-PlacePublicKey:
              command: !Sub |

                # Force new line in the log
                echo " "

                # Wait for the key and save it when ready
                param=$(aws ssm describe-parameters --region $AWS_REGION --query "Parameters[].Name" --output text --filters "Key=Name,Values=/$MAIN_STACK/datarobot-public-key")

                while [ ! -n "$param" ]; do
                  sleep 10
                  echo "Waiting 5 seconds for the key to arrive"
                  param=$(aws ssm describe-parameters --region $AWS_REGION --query "Parameters[].Name" --output text --filters "Key=Name,Values=/$MAIN_STACK/datarobot-public-key")
                done

                # Get the /$MAIN_STACK/datarobot-public-key
                key=$(aws ssm get-parameter --name "/$MAIN_STACK/datarobot-public-key" --region $AWS_REGION --query 'Parameter.Value' | cut -d "\"" -f2)

                # Append the public key to the auth keys file
                echo "$key" | tee -a /home/datarobot/.ssh/authorized_keys
                echo "Saved to /home/datarobot/.ssh/authorized_keys"
              ignoreErrors: "false"
              env:
                AWS_REGION: !Ref Region
                MAIN_STACK: !Ref MainStack

            4a-SetNodeCrontabSet:
              command: !Sub |

                # Force new line in the log
                echo " "

                my_name=$(aws ec2 describe-instances --region us-east-1 --instance-ids $(curl --silent http://169.254.169.254/latest/meta-data/instance-id) --query "Reservations[*].Instances[*].[Tags[?Key=='Name'].Value]" --output text)
                my_name=${!my_name##*-}

                if [[ $my_name == ModelingNode* ]]; then
                  echo "Skipping Modeling nodes"
                else
                  echo 'Adding ($SCHEDULE) cronjob syncing /opt/datarobot/data to s3://$S3_BUCKET/$BACKUP_DIR/data/$my_name'                  
                  printf '%s docker stop $(docker ps -aq) && aws s3 sync --no-progress /opt/datarobot/data s3://%s/%s/data/%s && docker start $(docker ps -aq)\n' "$SCHEDULE" "$S3_BUCKET" "$BACKUP_DIR" "$my_name" | tee -a /var/spool/cron/datarobot
                fi
              test: "test ${USE_S3_BACKUPS} = 'true'"
              ignoreErrors: "false"
              env:
                DR_VERSION: !Ref DRVersion
                BACKUP_DIR: !Ref BackupDir
                S3_BUCKET: !Ref InstallationBucket
                SCHEDULE: !Ref S3BucketBackupSchedule
                USE_S3_BACKUPS: !Ref UseS3BucketBackups

            4b-TagTheDataVolumes:
              command: !Sub |

                # Force new line in the log
                echo " "

                # Get the instance id
                instance_id=$(curl --silent http://169.254.169.254/latest/meta-data/instance-id)

                # Find the data volume id
                data_valume_id=$(aws ec2 describe-instances --region $AWS_REGION --instance-id $instance_id --output text --query "Reservations[0].Instances[0].BlockDeviceMappings[1].Ebs.VolumeId")

                # Get the node name
                node_name=$(aws ec2 describe-instances --region $AWS_REGION --instance-id $instance_id --output text --query "Reservations[].Instances[].[Tags[?Key=='Name'].Value[]]" | sed "s/^$MAIN_STACK-//")

                # Apply the tags to the instance
                aws ec2 create-tags --resources $data_valume_id --region $AWS_REGION --tags "Key=Name,Value=$node_name-DataVolume" "Key=MainStack,Value=$MAIN_STACK" "Key=NodeType,Value=DataRobot"
                echo "Tagged data volume :> '$data_valume_id' on node :> $node_name"

              test: "test ${TAG_DATA_VOL} = 'true'"
              ignoreErrors: "false"
              env:
                AWS_REGION: !Ref Region
                MAIN_STACK: !Ref MainStack
                TAG_DATA_VOL: !Ref TagDataVol

            4c-CreateSelfSignedCert:
              command: !Sub |
                # Create self signed keys

                # directories will not exist most likely since app node has not caught up and created
                mkdir /opt/datarobot/DataRobot-$DR_VERSION
                mkdir /opt/datarobot/DataRobot-$DR_VERSION/certs

                servername=$(uname -n)
                openssl genrsa -out /opt/datarobot/DataRobot-$DR_VERSION/certs/cert.key 2048
                openssl req -new -key /opt/datarobot/DataRobot-$DR_VERSION/certs/cert.key -out /opt/datarobot/DataRobot-$DR_VERSION/certs/cert.csr -subj '/CN=$servername'
                openssl x509 -req -days 365 -in /opt/datarobot/DataRobot-$DR_VERSION/certs/cert.csr -signkey /opt/datarobot/DataRobot-$DR_VERSION/certs/cert.key -out /opt/datarobot/DataRobot-$DR_VERSION/certs/cert.pem
                sudo chmod 444 /opt/datarobot/DataRobot-$DR_VERSION/certs/*

              ignoreErrors: "true"
              env:
                DR_VERSION: !Ref DRVersion
                
                

            5a-CreateAMI:
              command: !Sub |

                # Build AMI if correct type
                echo "Will build an AMI if modelingonly node"
                
                my_name=$(aws ec2 describe-instances --region $AWS_REGION --instance-ids $(curl --silent http://169.254.169.254/latest/meta-data/instance-id) --query "Reservations[*].Instances[*].[Tags[?Key=='Name'].Value]" --output text)
                my_name=${!my_name##*-}

                echo "My name is ($my_name)"
                
                if [[ $my_name == ModelingOnly* ]]; then

                  # cleanup drami from other runs of quickstart so drami key version remains 1
                  param=$(aws ssm describe-parameters --region $AWS_REGION --query "Parameters[].Name" --output text --filters "Key=Name,Values=drami")
                  if [ -n "$param"]; then
                    aws ssm delete-parameter --region $AWS_REGION --name "drami"
                  else
                    echo " no key to cleanup"
                  fi

                  #Wait to see a spare key from app node indicating that install is complete
                  param=$(aws ssm describe-parameters --region $AWS_REGION --query "Parameters[].Name" --output text --filters "Key=Name,Values=/$MAIN_STACK/datarobot-ami-key")
                  while [ ! -n "$param" ]; do
                    sleep 30
                    echo "Waiting 30 seconds for the spare ami key to arrive"
                    param=$(aws ssm describe-parameters --region $AWS_REGION --query "Parameters[].Name" --output text --filters "Key=Name,Values=/$MAIN_STACK/datarobot-ami-key")
                  done

                  echo "Creating AMI for ModelingOnly Node"
                  my_instance_id=$(curl --silent http://169.254.169.254/latest/meta-data/instance-id)
                  my_ami_name=$my_name
                  my_ami_name+="$(tr -dc A-Za-z0-9 </dev/urandom | head -c 8 ; echo '')"
                  my_ami_json=$(aws ec2 create-image --instance-id $my_instance_id --name $my_ami_name --no-reboot)
                  # my_ami_json will return a json immedately like: { "ImageId": "ami-########"}
                  my_ami=$(echo "$my_ami_json" | python3 -c "import sys, json; print(json.load(sys.stdin)['ImageId'])")
                  
                  echo "AMI created is ($my_ami)"

                  # delete the key used as a signal
                  aws ssm delete-parameter --region $AWS_REGION --name "/$MAIN_STACK/datarobot-ami-key"

                  # code to wait for AMI to be done
                  lc_count=$(aws ec2 describe-images --region $AWS_REGION --image-ids $my_ami | grep available | wc -l)
                  while [ $lc_count -lt 1 ]; do
                    echo "lc = $lc_count"
                    sleep 30
                    lc_count=$(aws ec2 describe-images --region $AWS_REGION --image-ids $my_ami | grep available | wc -l)
                  done

                  # write ami to ssm
                  aws ssm put-parameter --name "drami" --value "$my_ami" --region $AWS_REGION --type String --overwrite

                  # flag URL that AMI is complete
                  curl -X PUT -H 'Content-Type:' --data-binary '{"Status" : "SUCCESS","Reason" : "Configuration Complete","UniqueId" : "IDAMIDONE","Data" : "AMI Complete"}' "$AMI_SIGNAL"

                  # shutdown this instance - not in a scaling group

                  # put this back in for final version
                  # sudo /usr/sbin/shutdown +120
 
                else
                  echo "Skipping nodes other than ModelingOnly"
                fi
              
              ignoreErrors: "true"
              env:
                AWS_REGION: !Ref Region
                MAIN_STACK: !Ref MainStack
                TAG_DATA_VOL: !Ref TagDataVol
                AMI_SIGNAL: !Ref AmiSignalHandle

    Properties: 
      ImageId:  !Ref ImageId
      InstanceType: !Ref InstanceType
      IamInstanceProfile: !Ref EC2InstanceSecurityProfile
      KeyName: !Ref SSHKey
      Monitoring: true
      EbsOptimized: true
      BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
            DeleteOnTermination: true
            VolumeType: gp3
            VolumeSize: !Ref NodeVolumeSize
        - DeviceName: /dev/sdb
          Ebs:
            DeleteOnTermination: true
            VolumeType: gp3
            Encrypted: !Ref Encrypted
            KmsKeyId: !If [ "UseKey", !Ref "EncryptionKey", !Ref "AWS::NoValue" ]
            VolumeSize: !Ref DataVolumeSize
      NetworkInterfaces:
        - DeviceIndex: "0"
          AssociatePublicIpAddress: !Ref PublicIpAddress
          SubnetId: !Ref SubnetId
          GroupSet: 
            - !Ref EC2SecurityGroup

      UserData:
        Fn::Base64: !Sub |
          #!/bin/sh

          ### START Debug ###
          DRVersion=${DRVersion}
          MainStack=${MainStack}
          CurrentStackName=${AWS::StackName}
          Encrypted=${Encrypted}
          #EncryptionKey=${EncryptionKey}
          InstallationBucket=${InstallationBucket}
          InstanceType=${InstanceType}
          ImageId=${ImageId}
          LogServer=${LogServer}
          NodeVolumeSize=${NodeVolumeSize}
          PublicIpAddress=${PublicIpAddress}
          Region=${Region}
          SSHKey=${SSHKey}
          S3StorageDir=${StorageDir}
          SubnetId=${SubnetId}
          ### END Debug ###

          echo "yum install python3 python2"
          yum install -y python3
          python3 -m pip install --upgrade pip future boto3 argparse requests
          yum install -y python2
          python2 -m pip install --upgrade pip future boto3 argparse requests
          # Install aws cloudformation packages
          
          /usr/bin/easy_install-3 --script-dir /opt/aws/bin https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-py3-latest.tar.gz
          # ^^^ attempting to use latest for native python 3 support in centos 8.2

          # Start system provisioning
          /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource EC2Instance --region ${Region}

          # Signal template completion
          /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource EC2Instance --region ${Region}

Outputs:
  InstanceId:
    Description: "InstanceId of the newly created EC2 instance"
    Value: !Ref EC2Instance
    Export:
      Name: !Sub '${MainStack}-${ServerName}-EC2InstanceID'
  PrivateIP:
    Description: "PrivateIP IP address of the newly created EC2 instance"
    Value: !GetAtt EC2Instance.PrivateIp
